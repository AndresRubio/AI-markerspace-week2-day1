# AI Engineering Bootcamp: Detailed Schedule


>üß∞ For a comprehensive list of tools, go straight to the [Course Schedule](https://www.notion.so/AI-Engineering-Bootcamp-Detailed-Schedule-81d665f6286d4f298484cd10bf130eac?pvs=21)

# üßë‚Äçüíª¬†What is ‚ÄúAI Engineering?‚Äù

AI Engineering refers to the industry-relevant skills that data science and engineering teams need to successfully build, deploy, operate, and improve Large Language Model (LLM) applications in production environments.  

In practice, this requires understanding aspects of both prototyping and production deployments.  During the prototyping phase, Prompt Engineering, [Retrieval Augmented Generation (RAG)](https://www.youtube.com/playlist?list=PLrSHiQgy4VjFlWgcLHatJCBgfeE8smVyP), and [Fine-Tuning](https://www.youtube.com/playlist?list=PLrSHiQgy4VjGMzyXsSlvN-TjPaqFFsAGP) are all necessary tools to be able to understand and leverage. E.g.; 

![From [The New Stack and Ops for AI](https://www.youtube.com/watch?v=XGJNo8TpuVA)¬†by OpenAI on November 6, 2023.](https://i.imgur.com/4MWd8vy.png)

From [The New Stack and Ops for AI](https://www.youtube.com/watch?v=XGJNo8TpuVA)¬†by OpenAI on November 6, 2023.

When productionizing LLM application prototypes, there are many considerations to keep in mind when it comes to ensuring helpful, harmless, honest, reliable, and scalable solutions for your customers or stakeholders.

This course aims to cover the entire span of concepts and code necessary to prototype with LLMs as well as to deploy, operate, and improve them in production environments.  We focus on tools that are both seen as industry-standard and are leading the way at the open-source edge in 2024.

- This course combines aspects from our two previous courses, [LLM Engineering](https://maven.com/aimakerspace/llm-engineering) and [LLM Operations](https://maven.com/aimakerspace/llmops) üëá
    
    ## What is LLM Engineering?
    
    [LLM Engineering - The Foundations by Dr. Greg Loughnane and Chris "The LLM Wizard ü™Ñ" Alexiuk on Maven](https://maven.com/aimakerspace/llm-engineering)
    
    Large Language Model Engineering (LLM Engineering) refers to the emerging best-practices and tools for training, fine-tuning, and aligning LLMs prior to production deployment.
    
    LLM Engineering is the counterpart to ML Engineering (MLE) that focuses on LLM-specific techniques like prompt engineering, reinforcement learning, quantization, and other methods required by this new paradigm.
    
    ## What is LLM Ops?
    
    [LLM Ops - Large Language Models in Production by Dr. Greg Loughnane and Chris "The LLM Wizard ü™Ñ" Alexiuk  on Maven](https://maven.com/aimakerspace/llmops)
    
    Large Language Model Ops (LLM Ops, or LLMOps (as from¬†[WandB](https://docs.wandb.ai/guides/prompts)¬†and¬†[a16z](https://a16z.com/emerging-architectures-for-llm-applications/))) refers to the emerging best-practices, tooling, and improvement processes used to manage production LLM applications throughout the AI product lifecycle.
    
    LLM Ops is a subset of Machine Learning Operations (MLOps) that focuses on LLM-specific infrastructure and ops capabilities required to build, deploy, monitor, and scale complex LLM applications in production environments.
    
    Great new ideas include:
    
    - [The LLM Operating System](https://www.youtube.com/watch?v=zjkBMFhNj_g)¬†by¬†[Andrej Karpathy](https://twitter.com/karpathy)¬†on November 23, 2023
    - [The New Stack and Ops for AI](https://www.youtube.com/watch?v=XGJNo8TpuVA)¬†by OpenAI on November 6, 2023

# üìú AI Engineering Learning Targets

- **Prototype LLM Applications**
    - Building RAG Applications (e.g., Vector DBs, LLMs, Embedding Models)
    - Fine-Tuning LLMs & Embedding Models (e.g., Hugging Face, LoRA, Quantization)
    - Building Agent/Reasoning Applications (e.g., ReAct, function calling, external tools)
    - LLM Application Deployment (e.g., Chainlit, Hugging Face)
- **Production-Grade Applications**
    - RAG Evaluation and Improvement (e.g., RAGAS, advanced retrieval techniques)
    - Monitoring and Visibility Tooling (e.g., WandB, LangSmith)
    - Efficient Inference and Serving (e.g., LangServe, prompt caching)
    - Scaling LLM Applications (e.g., Amazon SageMaker)

# ü§î¬†Prerequisites

This course is designed for both aspiring AI Engineers and AI Engineering Leaders.  The former will, of course, be more interested in coding everything themselves, while leadership positions typically require only an understanding of high-level concepts, tools, and infrastructure.

We have had many students succeed by taking both paths through the course.

The ***bare minimum prerequisites** are:*

- Working knowledge of how to run Python code in Jupyter notebooks
- A functional understanding of how machine learning and deep learning models are trained.

If you are still **not at this level**, you should with the Machine Learning Specialization from Deeplearning.ai.  It will get you up to speed on both aspects.

[Machine Learning Specialization - DeepLearning.AI](https://www.deeplearning.ai/courses/machine-learning-specialization/)

To build, ship, and share LLM applications, you will also need to understand the following tools:

- Git/GitHub, Terminal/CLI, Web Frameworks (e.g., FastAPI), and Docker

Please start testing your knowledge by setting up your LLM application development environment before coming to class:

# üèÜ **Grading and Certification**

To become **AI-Makerspace Certified**, which will open you up to additional opportunities for full and part-time work within our community and network, you must:

1. Complete all project assignments.
2. Complete a project and present during Demo Day.
3. Receive at least an 85% total grade in the course.

If you do not complete all assignments, participate in Demo Day, or maintain a high-quality standard of work, you may still be eligible for a *certificate of completion* if you miss no more than 2 live sessions.

### **[Course Page on Maven](https://maven.com/aimakerspace/ai-eng-bootcamp)**
